ë””ì‹œ í¬ë¡¤ë§ + êµ¬ê¸€ì‹œíŠ¸ ìë™ ì €ì¥ (ë³µë¶™ìš©)
# ============================================================
# [Colab í†µí•©] ë””ì‹œì¸ì‚¬ì´ë“œ í¬ë¡¤ë§(ë‚ ì§œ ì…ë ¥) -> Google Sheet ìë™ ì €ì¥
# - ë³¸ë¬¸ ì¶”ì¶œ(ëª¨ë°”ì¼ ê°•ì œ) í¬í•¨
# - URL í™”ì´íŠ¸/ë¸”ë™ë¦¬ìŠ¤íŠ¸ í•„í„° í¬í•¨
# - ì¤‘ë³µ ì œê±°(URL ê¸°ì¤€) + ì»¤ìŠ¤í…€ ì •ë ¬ í¬í•¨
# - gspread DeprecationWarning ì œê±°(ìµœì‹  update ê·œê²©)
# - pandas Categorical ë¯¸ì‚¬ìš©(ì—…ë¡œë“œ/ê²°ì¸¡ì¹˜ TypeError ë°©ì§€)
# ============================================================

!pip -q install gspread google-auth google-auth-oauthlib google-auth-httplib2 pandas bs4 lxml

import re
import time
import random
import requests
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import quote, urlparse, parse_qs
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

import gspread
from google.colab import auth
from google.auth import default


# =========================
# 1) Google Sheet ì„¤ì • (URL ê³ ì •)
# =========================
SPREADSHEET_URL = "https://docs.google.com/spreadsheets/d/1RM3VNmM8_mW_MDCzp8lsko38KFVE5-5Hq5ImpLeYeQU/edit?gid=0#gid=0"
INPUT_SHEET_NAME = "ğŸ“‚ ì›ë³¸_ë°ì´í„°_ì…ë ¥"
HEADERS = ["ì›”", "ì¼", "ì»¤ë®¤ë‹ˆí‹°", "íƒ€ì‚¬ëª…/í‚¤ì›Œë“œ", "ì œëª©", "ë³¸ë¬¸(ë‚´ìš©)", "URL"]


# =========================
# 2) í¬ë¡¤ë§ ì„¤ì •
# =========================
SEARCH_KEYWORDS = [
    "ëŸ¬ì…€", "ëŒ€ì„±", "ì´íˆ¬ìŠ¤", "ë©”ê°€", "ì¬ì¢…",
    "ê´€ë¦¬í˜•ë…ì„œì‹¤", "ê´€ë…", "ì‹œëŒ€ì¸ì¬", "ë…í•™ì¬ìˆ˜", "ê¸°ìˆ™í•™ì›"
]

CUSTOM_SORT_ORDER = [
    "ëŸ¬ì…€", "ëŒ€ì„±", "ì´íˆ¬ìŠ¤", "ë©”ê°€", "ì¬ì¢…",
    "ê´€ë¦¬í˜•ë…ì„œì‹¤", "ê´€ë…", "ì‹œëŒ€ì¸ì¬", "ë…í•™ì¬ìˆ˜", "ê¸°ìˆ™í•™ì›", "ê¸°íƒ€"
]
SORT_RANK = {k: i for i, k in enumerate(CUSTOM_SORT_ORDER)}

TARGET_URL_KEYWORDS = [
    "russelmg", "hanmath", "itall", "exam_new2",
    "csatlecture", "sdijn", "dshw", "etoos",
    "kanganedu", "hwatwo", "waterlee1"
]

EXCLUDE_URL_KEYWORDS = [
    "whiskey", "americanbasketball", "jumbos", "jworg",
    "volleyballman", "nasdaq", "stockus", "tenbagger",
    "krstock", "sls", "loleague1", "of", "formula1",
    "dow100", "m2liquidity", "w_entertainer", "bigbangvip",
    "aichatting", "flowerroad", "hiphop_new1", "daesung", "blackwhites2"
]

FETCH_BODY = True
BODY_MAX_CHARS = 5000
POST_DELAY = (0.5, 1.2)
MAX_PAGES_PER_KEYWORD = 50


# =========================
# 3) Google Sheet ìœ í‹¸ (ê²½ê³  ì œê±° + append)
# =========================
def connect_gsheet():
    auth.authenticate_user()
    creds, _ = default()
    return gspread.authorize(creds)

def extract_spreadsheet_id(url: str) -> str:
    m = re.search(r"/spreadsheets/d/([a-zA-Z0-9-_]+)", url)
    if not m:
        raise ValueError("ìŠ¤í”„ë ˆë“œì‹œíŠ¸ IDë¥¼ URLì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return m.group(1)

def open_spreadsheet(gc: gspread.Client, url: str):
    sid = extract_spreadsheet_id(url)
    return gc.open_by_key(sid)

def get_or_create_worksheet(sh, title: str, rows: int = 2000, cols: int = 20):
    try:
        ws = sh.worksheet(title)
    except gspread.WorksheetNotFound:
        ws = sh.add_worksheet(title=title, rows=rows, cols=cols)
    return ws

def ensure_header(ws, headers):
    current = ws.row_values(1)
    if current[:len(headers)] != headers:
        ws.resize(rows=max(ws.row_count, 2), cols=max(ws.col_count, len(headers)))
        ws.update([headers], "A1")  # âœ… values ë¨¼ì €, range ë‚˜ì¤‘
        try:
            ws.format("A1:G1", {"textFormat": {"bold": True}, "horizontalAlignment": "CENTER"})
        except Exception:
            pass

def append_dataframe(ws, df: pd.DataFrame, headers):
    # ì»¬ëŸ¼ ì •í•©ì„±
    for h in headers:
        if h not in df.columns:
            df[h] = ""
    df2 = df[headers].copy()

    # ë¬¸ìì—´í™” + NaN ì•ˆì „ ì²˜ë¦¬
    for col in df2.columns:
        df2[col] = df2[col].astype(str)
    df2 = df2.fillna("")

    # append ì‹œì‘ í–‰
    last_row = len(ws.get_all_values())  # í—¤ë” í¬í•¨
    start_row = last_row + 1 if last_row >= 1 else 1

    values = df2.values.tolist()
    if not values:
        print("ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    cell_range = f"A{start_row}"
    ws.update(values, cell_range)  # âœ… values ë¨¼ì €, range ë‚˜ì¤‘
    print(f"ì—…ë¡œë“œ ì™„ë£Œ: {len(values)}í–‰ â†’ {ws.title}!{cell_range}ë¶€í„°")


# =========================
# 4) í¬ë¡¤ë§ ìœ í‹¸
# =========================
def clean_text(text: str) -> str:
    if not text:
        return ""
    text = text.strip()
    text = re.sub(r'[\u200b\u3000]', ' ', text)
    text = re.sub(r'[\r\n]+', '\n', text)
    return text

def build_session() -> requests.Session:
    s = requests.Session()
    retry = Retry(
        total=3, backoff_factor=1,
        status_forcelist=[500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    s.mount("https://", HTTPAdapter(max_retries=retry))
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Connection": "keep-alive",
        "Referer": "https://m.dcinside.com/"
    })
    return s

def is_valid_url(link: str) -> bool:
    if not link:
        return False
    for exc in EXCLUDE_URL_KEYWORDS:
        if exc in link:
            return False
    if TARGET_URL_KEYWORDS:
        return any(tgt in link for tgt in TARGET_URL_KEYWORDS)
    return True

def parse_id_no(link: str):
    try:
        u = urlparse(link)
        q = parse_qs(u.query)
        gid = q.get("id", [None])[0]
        no = q.get("no", [None])[0]

        if not gid or not no:
            parts = [p for p in u.path.split('/') if p]
            if len(parts) >= 3 and parts[0] == "board":
                gid, no = parts[1], parts[2]
        return gid, no
    except:
        return None, None

def convert_to_mobile_url(link: str) -> str:
    gid, no = parse_id_no(link)
    if gid and no:
        return f"https://m.dcinside.com/board/{gid}/{no}"
    return link

def extract_body_mobile(html: str) -> str:
    if not html:
        return ""
    if "ì‚­ì œëœ ê²Œì‹œë¬¼" in html:
        return "[ì‚­ì œëœ ê²Œì‹œë¬¼]"
    if "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²Œì‹œë¬¼" in html:
        return "[ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²Œì‹œë¬¼]"

    soup = BeautifulSoup(html, "lxml")
    selectors = ["div.thum-txt", "div.thum-txtin", "div.writing_view_box", "div.view_content_wrap"]

    body_el = None
    for sel in selectors:
        body_el = soup.select_one(sel)
        if body_el:
            break
    if not body_el:
        return ""

    for tag in body_el.select("script, style, img, video, iframe, .recomm_btn_box"):
        tag.decompose()

    text = body_el.get_text("\n", strip=True)
    return clean_text(text)[:BODY_MAX_CHARS]

def fetch_post_body(session: requests.Session, link: str) -> str:
    url = convert_to_mobile_url(link)
    try:
        time.sleep(random.uniform(*POST_DELAY))
        r = session.get(url, timeout=10)
        if r.status_code == 200:
            return extract_body_mobile(r.text)
        return ""
    except:
        return ""

def parse_date(date_str: str) -> datetime:
    now = datetime.now()
    s = (date_str or "").strip()

    m = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', s)
    if m:
        y, mo, d = map(int, m.groups())
        return datetime(y, mo, d)

    m = re.search(r'(\d{1,2})\.(\d{1,2})', s)
    if m:
        mo, d = map(int, m.groups())
        dt = datetime(now.year, mo, d)
        if dt > now:
            dt = dt.replace(year=now.year - 1)
        return dt

    if ":" in s:
        return datetime(now.year, now.month, now.day)

    return now


# =========================
# 5) ë‚ ì§œ ì…ë ¥ â†’ í¬ë¡¤ë§ â†’ êµ¬ê¸€ì‹œíŠ¸ ì €ì¥
# =========================
def get_user_cutoff_date():
    while True:
        print("\n" + "=" * 60)
        print("ì˜ˆì‹œ: 2026-01-05 ì…ë ¥ ì‹œ, 2026-01-05(í¬í•¨) ~ í˜„ì¬(ì˜¤ëŠ˜)ê¹Œì§€ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
        date_input = input("ğŸ“… ì–¸ì œê¹Œì§€(í•´ë‹¹ ë‚ ì§œ í¬í•¨ ì´í›„)ë¥¼ ìˆ˜ì§‘í• ê¹Œìš”? (YYYY-MM-DD)\nğŸ‘‰ ì…ë ¥: ").strip()
        if not date_input:
            print("âš ï¸ ë‚ ì§œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            continue
        try:
            return datetime.strptime(date_input, "%Y-%m-%d")
        except ValueError:
            print("âŒ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

def run_crawl(cutoff_dt: datetime) -> pd.DataFrame:
    session = build_session()
    all_results = []

    print("\n" + "=" * 70)
    print(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘ | cutoff(í¬í•¨): {cutoff_dt.strftime('%Y-%m-%d')}")
    print(f"ğŸ” í‚¤ì›Œë“œ: {', '.join(SEARCH_KEYWORDS)}")
    print("=" * 70)

    for keyword in SEARCH_KEYWORDS:
        print(f"\nâ–¶ï¸ [{keyword}] ìˆ˜ì§‘ ì¤‘...")
        page = 1
        keep_crawling = True

        while keep_crawling and page <= MAX_PAGES_PER_KEYWORD:
            encoded = quote(keyword)
            url = f"https://search.dcinside.com/post/p/{page}/sort/date/q/{encoded}"

            try:
                time.sleep(random.uniform(*POST_DELAY))
                search_headers = session.headers.copy()
                search_headers["User-Agent"] = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36"

                resp = session.get(url, headers=search_headers, timeout=10)
                if resp.status_code != 200:
                    break

                soup = BeautifulSoup(resp.text, "html.parser")
                posts = soup.select(".sch_result_list > li")
                if not posts:
                    break

                valid_count = 0
                for post in posts:
                    date_tag = post.select_one("span.date_time")
                    title_tag = post.select_one("a.tit_txt")
                    if not date_tag or not title_tag:
                        continue

                    post_date = parse_date(date_tag.text)

                    # cutoff ì´ì „(ë” ê³¼ê±°)ë©´ ì •ì§€
                    if post_date < cutoff_dt:
                        print(f"   ğŸ›‘ {post_date.strftime('%Y-%m-%d')} ë°œê²¬ â†’ cutoff ë„ë‹¬ë¡œ ì •ì§€")
                        keep_crawling = False
                        break

                    title = clean_text(title_tag.text)
                    link = title_tag.get("href", "")

                    if not is_valid_url(link):
                        continue

                    body = fetch_post_body(session, link) if FETCH_BODY else ""

                    all_results.append({
                        "ì›”": str(post_date.month),
                        "ì¼": str(post_date.day),
                        "ì»¤ë®¤ë‹ˆí‹°": "ë””ì‹œì¸ì‚¬ì´ë“œ",
                        "íƒ€ì‚¬ëª…/í‚¤ì›Œë“œ": keyword,
                        "ì œëª©": title,
                        "ë³¸ë¬¸(ë‚´ìš©)": body,
                        "URL": link,
                        "_sort_date": post_date
                    })
                    valid_count += 1

                print(f"   âœ… {page}í˜ì´ì§€: {valid_count}ê±´")
                page += 1

            except Exception as e:
                print(f"   âŒ ì—ëŸ¬: {e}")
                break

    if not all_results:
        return pd.DataFrame(columns=HEADERS)

    df = pd.DataFrame(all_results)

    # ì¤‘ë³µ ì œê±°: URL ê¸°ì¤€(ì•ˆì •)
    df.drop_duplicates(subset=["URL"], keep="first", inplace=True)

    # ì •ë ¬: ë‚ ì§œ ì˜¤ë¦„ì°¨ìˆœ + í‚¤ì›Œë“œ ì§€ì • ìˆœì„œ
    df["_kw_rank"] = df["íƒ€ì‚¬ëª…/í‚¤ì›Œë“œ"].map(SORT_RANK).fillna(SORT_RANK.get("ê¸°íƒ€", 999)).astype(int)
    df =A
    df = df.sort_values(by=["_sort_date", "_kw_rank"], ascending=[True, True])

    df.drop(columns=["_sort_date", "_kw_rank"], inplace=True, errors="ignore")

    for c in HEADERS:
        if c not in df.columns:
            df[c] = ""
    df = df[HEADERS].copy().fillna("")

    return df

def main():
    cutoff_dt = get_user_cutoff_date()
    df_result = run_crawl(cutoff_dt)

    print("\n" + "=" * 70)
    print(f"ğŸ“Œ í¬ë¡¤ë§ ê²°ê³¼: {len(df_result)}ê±´")
    print("=" * 70)

    if len(df_result) == 0:
        print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    gc = connect_gsheet()
    sh = open_spreadsheet(gc, SPREADSHEET_URL)
    ws = get_or_create_worksheet(sh, INPUT_SHEET_NAME)
    ensure_header(ws, HEADERS)
    append_dataframe(ws, df_result, HEADERS)

    print("\nâœ… ì™„ë£Œ: í¬ë¡¤ë§ â†’ êµ¬ê¸€ì‹œíŠ¸ ì €ì¥")
    print("ì‹œíŠ¸ ì´ë¦„:", sh.title)
    print("íƒ­ ì´ë¦„:", ws.title)

main()
