# ë””ì‹œ + ì˜¤ë¥´ë¹„ + ë„¤ì´ë²„ì¹´í˜ í†µí•© í¬ë¡¤ë§ + êµ¬ê¸€ì‹œíŠ¸ ìë™ ì €ì¥
# ============================================================
# [Colab í†µí•©] ë‹¤ì¤‘ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ -> Google Sheet ìë™ ì €ì¥
# - ëŒ€ìƒ: ë””ì‹œì¸ì‚¬ì´ë“œ, ì˜¤ë¥´ë¹„, ìˆ˜ë§Œíœ˜(ë„¤ì´ë²„), í¬ë§Œí•œ(ë„¤ì´ë²„)
# - ê¸°ëŠ¥: ì‚¬ì´íŠ¸ë³„ ê²€ìƒ‰, ë³¸ë¬¸ ì¶”ì¶œ, ë‚ ì§œ í•„í„°ë§, í†µí•© ì €ì¥
# ============================================================

!pip -q install gspread google-auth google-auth-oauthlib google-auth-httplib2 pandas bs4 lxml

import re
import time
import random
import requests
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import quote, urlparse, parse_qs
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

import gspread
from google.colab import auth
from google.auth import default
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


# =========================
# 1) ì„¤ì • (URL ë° í‚¤ì›Œë“œ)
# =========================
SPREADSHEET_URL = "https://docs.google.com/spreadsheets/d/1RM3VNmM8_mW_MDCzp8lsko38KFVE5-5Hq5ImpLeYeQU/edit?gid=388396824#gid=388396824"
INPUT_SHEET_NAME = "ğŸ“‚ ì›ë³¸_ë°ì´í„°_ì…ë ¥"
HEADERS = ["ì›”", "ì¼", "ì»¤ë®¤ë‹ˆí‹°", "íƒ€ì‚¬ëª…/í‚¤ì›Œë“œ", "ì œëª©", "ë³¸ë¬¸(ë‚´ìš©)", "URL"]

SEARCH_KEYWORDS = [
    "ëŸ¬ì…€", "ëŒ€ì„±", "ì´íˆ¬ìŠ¤", "ë©”ê°€", "ì¬ì¢…",
    "ê´€ë¦¬í˜•ë…ì„œì‹¤", "ê´€ë…", "ì‹œëŒ€ì¸ì¬", "ë…í•™ì¬ìˆ˜", "ê¸°ìˆ™í•™ì›"
]

# ë„¤ì´ë²„ ì¹´í˜ ì •ë³´ (cafeId)
NAVER_CAFES = {
    "ìˆ˜ë§Œíœ˜": "10139480",
    "í¬ë§Œí•œ": "14387405"
}

# ê³µí†µ ì„¤ì •
FETCH_BODY = True
BODY_MAX_CHARS = 5000
POST_DELAY = (1.5, 3.0)  # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•´ ë”œë ˆì´ ìƒí–¥
MAX_PAGES_PER_KEYWORD = 10 

# URL í•„í„°ë§ (ë””ì‹œìš©)
TARGET_URL_KEYWORDS = ["russelmg", "hanmath", "itall", "exam_new2", "csatlecture", "sdijn", "dshw", "etoos", "kanganedu", "hwatwo", "waterlee1"]
EXCLUDE_URL_KEYWORDS = ["whiskey", "americanbasketball", "jumbos", "jworg", "volleyballman", "nasdaq", "stockus", "tenbagger", "krstock", "sls", "loleague1", "of", "formula1", "dow100", "m2liquidity", "w_entertainer", "bigbangvip", "aichatting", "flowerroad", "hiphop_new1", "daesung", "blackwhites2"]


# =========================
# 2) ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =========================
def connect_gsheet():
    auth.authenticate_user()
    creds, _ = default()
    return gspread.authorize(creds)

def extract_spreadsheet_id(url: str) -> str:
    m = re.search(r"/spreadsheets/d/([a-zA-Z0-9-_]+)", url)
    if not m: raise ValueError("ìŠ¤í”„ë ˆë“œì‹œíŠ¸ IDë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return m.group(1)

def extract_gid(url: str) -> str:
    """URLì—ì„œ gid íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    m = re.search(r"gid=(\d+)", url)
    return m.group(1) if m else None

def ensure_header(ws, headers):
    try:
        current = ws.row_values(1)
        if not current or current[:len(headers)] != headers:
            ws.update(range_name="A1", values=[headers])
    except Exception as e:
        logger.debug(f"í—¤ë” í™•ì¸ ì¤‘ ì˜¤ë¥˜(ë¬´ì‹œ ê°€ëŠ¥): {e}")

def build_session() -> requests.Session:
    s = requests.Session()
    # 500 ì—ëŸ¬ ë“±ì— ëŒ€í•œ ì¬ì‹œë„ ë¡œì§ ê°•í™”
    retry = Retry(
        total=5, 
        backoff_factor=1, 
        status_forcelist=[500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    s.mount("https://", HTTPAdapter(max_retries=retry))
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Cache-Control": "max-age=0",
        "Connection": "keep-alive"
    })
    return s

def clean_text(text: str) -> str:
    if not text: return ""
    text = re.sub(r'[\u200b\u3000]', ' ', text.strip())
    return re.sub(r'[\r\n]+', '\n', text)

def parse_date(date_str: str) -> datetime:
    now = datetime.now()
    s = (date_str or "").strip()
    if not s: return now
    try:
        # "14:24" ë“± ì‹œê°„ í˜•ì‹
        if ':' in s and len(s) <= 5: 
            return datetime(now.year, now.month, now.day)
        # "2ì‹œê°„ ì „" ë“± ìƒëŒ€ í˜•ì‹
        if 'ì‹œê°„ ì „' in s:
            hours = int(re.search(r'(\d+)', s).group(1))
            return now - timedelta(hours=hours)
        if 'ë¶„ ì „' in s:
            return now
        # "2024.12.30" ë˜ëŠ” "24.12.30"
        m = re.search(r'(\d{2,4})\.(\d{1,2})\.(\d{1,2})', s)
        if m:
            y = int(m.group(1))
            if y < 100: y += 2000
            return datetime(y, int(m.group(2)), int(m.group(3)))
        # "12.30"
        m = re.search(r'(\d{1,2})\.(\d{1,2})', s)
        if m:
            dt = datetime(now.year, int(m.group(1)), int(m.group(2)))
            return dt if dt <= now else dt.replace(year=now.year - 1)
    except: pass
    return now


# =========================
# 3) ì‚¬ì´íŠ¸ë³„ ìˆ˜ì§‘ ë¡œì§
# =========================

# 3-1) ë””ì‹œì¸ì‚¬ì´ë“œ
def fetch_dc_inside(session, keyword, cutoff_dt):
    results = []
    print(f"   [ë””ì‹œì¸ì‚¬ì´ë“œ] í‚¤ì›Œë“œ: {keyword}")
    for page in range(1, MAX_PAGES_PER_KEYWORD + 1):
        url = f"https://search.dcinside.com/post/p/{page}/sort/date/q/{quote(keyword)}"
        try:
            time.sleep(random.uniform(*POST_DELAY))
            resp = session.get(url, timeout=12)
            if resp.status_code != 200: break
            soup = BeautifulSoup(resp.text, "html.parser")
            posts = soup.select(".sch_result_list > li")
            if not posts: break
            
            for post in posts:
                date_tag, title_tag = post.select_one("span.date_time"), post.select_one("a.tit_txt")
                if not date_tag or not title_tag: continue
                p_date = parse_date(date_tag.text)
                if p_date < cutoff_dt: return results
                
                link = title_tag.get("href", "")
                if any(x in link for x in EXCLUDE_URL_KEYWORDS): continue
                if TARGET_URL_KEYWORDS and not any(x in link for x in TARGET_URL_KEYWORDS): continue
                
                body = ""
                if FETCH_BODY:
                    try:
                        m_link = link
                        m = re.search(r"id=([^&]+)&no=(\d+)", link)
                        if m: m_link = f"https://m.dcinside.com/board/{m.group(1)}/{m.group(2)}"
                        b_resp = session.get(m_link, timeout=10)
                        b_soup = BeautifulSoup(b_resp.text, "lxml")
                        body_el = b_soup.select_one("div.thum-txt, div.writing_view_box, div.thum-txtin")
                        if body_el: body = clean_text(body_el.get_text("\n"))[:BODY_MAX_CHARS]
                    except: pass

                results.append({
                    "ì›”": str(p_date.month), "ì¼": str(p_date.day), "ì»¤ë®¤ë‹ˆí‹°": "ë””ì‹œì¸ì‚¬ì´ë“œ",
                    "íƒ€ì‚¬ëª…/í‚¤ì›Œë“œ": keyword, "ì œëª©": clean_text(title_tag.text), "ë³¸ë¬¸(ë‚´ìš©)": body, "URL": link
                })
        except Exception as e:
            logger.error(f"ë””ì‹œ ì—ëŸ¬: {e}"); break
    return results

# 3-2) ì˜¤ë¥´ë¹„
def fetch_orbi(session, keyword, cutoff_dt):
    results = []
    print(f"   [ì˜¤ë¥´ë¹„] í‚¤ì›Œë“œ: {keyword}")
    url = f"https://orbi.kr/search?q={quote(keyword)}&type=post"
    try:
        time.sleep(random.uniform(*POST_DELAY))
        # ì˜¤ë¥´ë¹„ëŠ” Refererê°€ ì¤‘ìš”í•  ìˆ˜ ìˆìŒ
        headers = session.headers.copy()
        headers.update({"Referer": "https://orbi.kr/"})
        resp = session.get(url, headers=headers, timeout=15)
        if resp.status_code != 200: return results
        
        soup = BeautifulSoup(resp.text, "html.parser")
        # 1. í‘œì¤€ ì…€ë ‰í„° ì‹œë„
        posts = soup.select("div.search-result div.item, div.post-list li, ul.post-list li")
        
        # 2. ì •ê·œì‹ ê¸°ë°˜ ìˆ˜ë™ íŒŒì‹± (SPA ëŒ€ì‘)
        # HTML ì†ŒìŠ¤ì— ë§í¬ê°€ í¬í•¨ë˜ì–´ ìˆëŠ” ê²½ìš° ì§ì ‘ ì¶”ì¶œ
        if not posts:
            found_patterns = re.findall(r'href="(/([0-9]{8,11}))"[^>]*>(.*?)</a>', resp.text)
            for link_part, post_id, title_raw in found_patterns:
                if post_id.startswith('000'): # ì˜¤ë¥´ë¹„ ê²Œì‹œê¸€ ID íŒ¨í„´
                    title = clean_text(re.sub(r'<[^>]+>', '', title_raw))
                    if not title or title in ['ê¸€ì“°ê¸°', 'ì´ìš©ì•½ê´€', 'ê°œì¸ì •ë³´']: continue
                    
                    results.append({
                        "ì›”": str(cutoff_dt.month), "ì¼": str(cutoff_dt.day), "ì»¤ë®¤ë‹ˆí‹°": "ì˜¤ë¥´ë¹„",
                        "íƒ€ì‚¬ëª…/í‚¤ì›Œë“œ": keyword, "ì œëª©": title, "ë³¸ë¬¸(ë‚´ìš©)": "", "URL": "https://orbi.kr" + link_part
                    })
            return results

        for post in posts:
            title_tag = post.select_one("a.title, a.tit, a")
            date_tag = post.select_one("span.date, span.time")
            if not title_tag: continue
            
            p_date = parse_date(date_tag.text) if date_tag else cutoff_dt
            if p_date < cutoff_dt: continue
            
            link = title_tag.get("href", "")
            if not link or "search" in link: continue
            if link and not link.startswith("http"): link = "https://orbi.kr" + link
            
            results.append({
                "ì›”": str(p_date.month), "ì¼": str(p_date.day), "ì»¤ë®¤ë‹ˆí‹°": "ì˜¤ë¥´ë¹„",
                "íƒ€ì‚¬ëª…/í‚¤ì›Œë“œ": keyword, "ì œëª©": clean_text(title_tag.text), "ë³¸ë¬¸(ë‚´ìš©)": "", "URL": link
            })
    except Exception as e:
        logger.error(f"ì˜¤ë¥´ë¹„ ì—ëŸ¬: {e}")
    return results

# 3-3) ë„¤ì´ë²„ ì¹´í˜ (ìˆ˜ë§Œíœ˜, í¬ë§Œí•œ) - ë¡œì§ ê³ ë„í™”
def fetch_naver_cafe(session, cafe_name, cafe_id, keyword, cutoff_dt):
    results = []
    print(f"   [{cafe_name}] í‚¤ì›Œë“œ: {keyword}")
    
    # í—¤ë” ë° ë¦¬í¼ëŸ¬ ê°•í™”
    cafe_headers = session.headers.copy()
    cafe_headers.update({
        "Referer": f"https://m.cafe.naver.com/ca-fe/web/cafes/{cafe_id}/search/articles?query={quote(keyword)}",
        "X-Cafe-Product": "m",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8"
    })

    # ëª¨ë°”ì¼ í†µí•© ê²€ìƒ‰ ê¸°ë°˜ ìš°íšŒ URL ì‹œë„ (ê°€ì¥ ì•ˆì •ì )
    url = f"https://m.cafe.naver.com/ArticleSearchList.nhn?search.cafeId={cafe_id}&search.query={quote(keyword)}&search.page=1"
    
    try:
        time.sleep(random.uniform(2.0, 4.0))
        resp = session.get(url, headers=cafe_headers, timeout=15)
        
        if resp.status_code != 200:
            # 2ì°¨ ì‹œë„: ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° ì¡°í•©
            url = f"https://m.cafe.naver.com/SectionArticleSearch.nhn?cafeId={cafe_id}&query={quote(keyword)}"
            resp = session.get(url, headers=cafe_headers, timeout=15)
            if resp.status_code != 200: return results
            
        soup = BeautifulSoup(resp.text, "html.parser")
        
        # ì •ê·œì‹ìœ¼ë¡œ ë§í¬ì™€ ì œëª© ì§ì ‘ ë§¤ì¹­í•˜ì—¬ SPA í˜•íƒœë¼ë„ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¶”ì¶œ
        # íŒ¨í„´: articles/12345678...
        links = re.findall(r'articles/(\d+)', resp.text)
        if links:
            # ì¤‘ë³µ ì œê±° ë° ìˆ˜ì§‘
            for aid in list(set(links)):
                results.append({
                    "ì›”": str(cutoff_dt.month), "ì¼": str(cutoff_dt.day), "ì»¤ë®¤ë‹ˆí‹°": cafe_name,
                    "íƒ€ì‚¬ëª…/í‚¤ì›Œë“œ": keyword, "ì œëª©": f"ê²Œì‹œê¸€(ID:{aid}) - ìƒì„¸ ë‚´ìš© í™•ì¸ ë¡œì§ í•„ìš”", "ë³¸ë¬¸(ë‚´ìš©)": "", 
                    "URL": f"https://m.cafe.naver.com/ca-fe/web/cafes/{cafe_id}/articles/{aid}"
                })
        
        # í‘œì¤€ BS4 íŒŒì‹± ì‹œë„ (ì •ì  HTMLì´ ë°˜í™˜ëœ ê²½ìš° ëŒ€ë¹„)
        posts = soup.select("li.board_box, ul.list_area > li, div.article_list_area li")
        for post in posts:
            title_el = post.select_one("strong.tit, span.tit, div.tit, strong")
            date_el = post.select_one("span.time, span.date")
            if not title_el: continue
            
            p_date = parse_date(date_el.text) if date_el else cutoff_dt
            if p_date < cutoff_dt: continue
            
            a_tag = post.select_one("a")
            if not a_tag: continue
            link = a_tag.get("href", "")
            if link and not link.startswith("http"): link = "https://m.cafe.naver.com" + link
            
            results.append({
                "ì›”": str(p_date.month), "ì¼": str(p_date.day), "ì»¤ë®¤ë‹ˆí‹°": cafe_name,
                "íƒ€ì‚¬ëª…/í‚¤ì›Œë“œ": keyword, "ì œëª©": clean_text(title_el.text), "ë³¸ë¬¸(ë‚´ìš©)": "", "URL": link
            })
            
    except Exception as e:
        logger.error(f"ë„¤ì´ë²„ ì¹´í˜({cafe_name}) ì—ëŸ¬: {e}")
    
    # URL ì¤‘ë³µ ì œê±°
    unique_results = []
    seen_urls = set()
    for res in results:
        if res['URL'] not in seen_urls:
            unique_results.append(res)
            seen_urls.add(res['URL'])
            
    return unique_results


# =========================
# 4) ë©”ì¸ ì‹¤í–‰ ë¡œì§
# =========================
def main():
    print("\nğŸ“… ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ ì…ë ¥ (YYYY-MM-DD)")
    date_input = input("ğŸ‘‰ ì…ë ¥: ").strip()
    try:
        cutoff_dt = datetime.strptime(date_input, "%Y-%m-%d")
    except:
        print("âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤."); return

    session = build_session()
    final_data = []

    for kw in SEARCH_KEYWORDS:
        logger.info(f"ğŸš€ [{kw}] í†µí•© ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ê°€ë™...")
        
        # ê° ì‚¬ì´íŠ¸ë³„ ìˆ˜ì§‘ ê¸°ëŠ¥ì„ ë…ë¦½ì ì¸ try-exceptë¡œ ê°ì‹¸ ì•ˆì •ì„± í™•ë³´
        try:
            final_data.extend(fetch_dc_inside(session, kw, cutoff_dt))
        except Exception as e:
            logger.error(f"ë””ì‹œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        try:
            final_data.extend(fetch_orbi(session, kw, cutoff_dt))
        except Exception as e:
            logger.error(f"ì˜¤ë¥´ë¹„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        for name, cid in NAVER_CAFES.items():
            try:
                final_data.extend(fetch_naver_cafe(session, name, cid, kw, cutoff_dt))
            except Exception as e:
                logger.error(f"{name} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    df = pd.DataFrame(final_data).drop_duplicates(subset=["URL"])
    print(f"\nâœ… í•„í„°ë§ í›„ ì´ {len(df)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ (ì¤‘ë³µ ì œì™¸)")
    
    if not df.empty:
        print("\nğŸ“ ìˆ˜ì§‘ëœ ë°ì´í„° ìƒ˜í”Œ (ìƒìœ„ 5ê±´):")
        for i, row in df.head(5).iterrows():
            print(f"   - [{row['ì»¤ë®¤ë‹ˆí‹°']}] {row['ì œëª©'][:40]}...")

    # êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥
    try:
        print("\nğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì‹œë„ ì¤‘...")
        gc = connect_gsheet()
        spreadsheet_id = extract_spreadsheet_id(SPREADSHEET_URL)
        target_gid = extract_gid(SPREADSHEET_URL)
        sh = gc.open_by_key(spreadsheet_id)
        
        # GIDê°€ ìˆìœ¼ë©´ IDë¡œ, ì—†ìœ¼ë©´ ì´ë¦„ìœ¼ë¡œ ì‹œíŠ¸ ì„ íƒ
        ws = None
        if target_gid:
            try:
                ws = sh.get_worksheet_by_id(int(target_gid))
                logger.info(f"ï¿½ GID({target_gid})ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œíŠ¸ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤: '{ws.title}'")
            except:
                logger.warning(f"âš ï¸ GID({target_gid})ë¡œ ì‹œíŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì´ë¦„('{INPUT_SHEET_NAME}')ìœ¼ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
        
        if not ws:
            ws = sh.worksheet(INPUT_SHEET_NAME)

        ensure_header(ws, HEADERS)
        
        # ë°ì´í„° ì—…ë¡œë“œ
        df_upload = df[HEADERS].fillna("").astype(str)
        if not df_upload.empty:
            data_to_append = df_upload.values.tolist()
            print(f"â¬†ï¸ '{ws.title}' ì‹œíŠ¸ì— {len(data_to_append)}ê±´ì˜ ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤...")
            # append_rowsê°€ ë” ì•ˆì „í•˜ê³  ë¹ ë¦„
            ws.append_rows(data_to_append, value_input_option='USER_ENTERED')
            logger.info(f"ğŸš€ êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ ì„±ê³µ! ({ws.title})")
        else:
            logger.info("âš ï¸ ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ ê³¼ì •ì—ì„œ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    main()
