# ë””ì‹œ + ì˜¤ë¥´ë¹„ + ë„¤ì´ë²„ì¹´í˜ í†µí•© í¬ë¡¤ë§ + êµ¬ê¸€ì‹œíŠ¸ ìë™ ì €ì¥
# ============================================================
# [Colab í†µí•©] ë‹¤ì¤‘ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ -> Google Sheet ìë™ ì €ì¥
# - ëŒ€ìƒ: ë””ì‹œì¸ì‚¬ì´ë“œ, ì˜¤ë¥´ë¹„, ìˆ˜ë§Œíœ˜(ë„¤ì´ë²„), í¬ë§Œí•œ(ë„¤ì´ë²„)
# - ê¸°ëŠ¥: ì‚¬ì´íŠ¸ë³„ ê²€ìƒ‰, ë³¸ë¬¸ ì¶”ì¶œ, ë‚ ì§œ í•„í„°ë§, í†µí•© ì €ì¥
# ============================================================

!pip -q install gspread google-auth google-auth-oauthlib google-auth-httplib2 pandas bs4 lxml

import re
import time
import random
import requests
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import quote, urlparse, parse_qs
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

import gspread
from google.colab import auth
from google.auth import default
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


# =========================
# 1) ì„¤ì • (URL ë° í‚¤ì›Œë“œ)
# =========================
SPREADSHEET_URL = "https://docs.google.com/spreadsheets/d/1RM3VNmM8_mW_MDCzp8lsko38KFVE5-5Hq5ImpLeYeQU/edit?gid=388396824#gid=388396824"
INPUT_SHEET_NAME = "ğŸ“‚ ì›ë³¸_ë°ì´í„°_ì…ë ¥"
HEADERS = ["ì›”", "ì¼", "ì»¤ë®¤ë‹ˆí‹°", "íƒ€ì‚¬ëª…/í‚¤ì›Œë“œ", "ì œëª©", "ë³¸ë¬¸(ë‚´ìš©)", "URL"]

SEARCH_KEYWORDS = [
    "ëŸ¬ì…€", "ëŒ€ì„±", "ì´íˆ¬ìŠ¤", "ë©”ê°€", "ì¬ì¢…",
    "ê´€ë¦¬í˜•ë…ì„œì‹¤", "ê´€ë…", "ì‹œëŒ€ì¸ì¬", "ë…í•™ì¬ìˆ˜", "ê¸°ìˆ™í•™ì›"
]

# ë„¤ì´ë²„ ì¹´í˜ ì •ë³´ (cafeId)
NAVER_CAFES = {
    "ìˆ˜ë§Œíœ˜": "10139480",
    "í¬ë§Œí•œ": "14387405"
}

# ê³µí†µ ì„¤ì •
FETCH_BODY = True
BODY_MAX_CHARS = 5000
POST_DELAY = (1.5, 3.0)  # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•´ ë”œë ˆì´ ìƒí–¥
MAX_PAGES_PER_KEYWORD = 10 

# URL í•„í„°ë§ (ë””ì‹œìš©)
TARGET_URL_KEYWORDS = ["russelmg", "hanmath", "itall", "exam_new2", "csatlecture", "sdijn", "dshw", "etoos", "kanganedu", "hwatwo", "waterlee1"]
EXCLUDE_URL_KEYWORDS = ["whiskey", "americanbasketball", "jumbos", "jworg", "volleyballman", "nasdaq", "stockus", "tenbagger", "krstock", "sls", "loleague1", "of", "formula1", "dow100", "m2liquidity", "w_entertainer", "bigbangvip", "aichatting", "flowerroad", "hiphop_new1", "daesung", "blackwhites2"]


# =========================
# 2) ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =========================
def connect_gsheet():
    auth.authenticate_user()
    creds, _ = default()
    return gspread.authorize(creds)

def extract_spreadsheet_id(url: str) -> str:
    m = re.search(r"/spreadsheets/d/([a-zA-Z0-9-_]+)", url)
    if not m: raise ValueError("ìŠ¤í”„ë ˆë“œì‹œíŠ¸ IDë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return m.group(1)

def extract_gid(url: str) -> str:
    """URLì—ì„œ gid íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    m = re.search(r"gid=(\d+)", url)
    return m.group(1) if m else None

def ensure_header(ws, headers):
    try:
        current = ws.row_values(1)
        if current[:len(headers)] != headers:
            ws.update([headers], "A1")
    except: pass

def build_session() -> requests.Session:
    s = requests.Session()
    # 500 ì—ëŸ¬ ë“±ì— ëŒ€í•œ ì¬ì‹œë„ ë¡œì§ ê°•í™”
    retry = Retry(
        total=5, 
        backoff_factor=1, 
        status_forcelist=[500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    s.mount("https://", HTTPAdapter(max_retries=retry))
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Cache-Control": "max-age=0",
        "Connection": "keep-alive"
    })
    return s

def clean_text(text: str) -> str:
    if not text: return ""
    text = re.sub(r'[\u200b\u3000]', ' ', text.strip())
    return re.sub(r'[\r\n]+', '\n', text)

def parse_date(date_str: str) -> datetime:
    now = datetime.now()
    s = (date_str or "").strip()
    if not s: return now
    try:
        # "14:24" ë“± ì‹œê°„ í˜•ì‹
        if ':' in s and len(s) <= 5: 
            return datetime(now.year, now.month, now.day)
        # "2ì‹œê°„ ì „" ë“± ìƒëŒ€ í˜•ì‹
        if 'ì‹œê°„ ì „' in s:
            hours = int(re.search(r'(\d+)', s).group(1))
            return now - timedelta(hours=hours)
        if 'ë¶„ ì „' in s:
            return now
        # "2024.12.30" ë˜ëŠ” "24.12.30"
        m = re.search(r'(\d{2,4})\.(\d{1,2})\.(\d{1,2})', s)
        if m:
            y = int(m.group(1))
            if y < 100: y += 2000
            return datetime(y, int(m.group(2)), int(m.group(3)))
        # "12.30"
        m = re.search(r'(\d{1,2})\.(\d{1,2})', s)
        if m:
            dt = datetime(now.year, int(m.group(1)), int(m.group(2)))
            return dt if dt <= now else dt.replace(year=now.year - 1)
    except: pass
    return now


# =========================
# 3) ì‚¬ì´íŠ¸ë³„ ìˆ˜ì§‘ ë¡œì§
# =========================

# 3-1) ë””ì‹œì¸ì‚¬ì´ë“œ
def fetch_dc_inside(session, keyword, cutoff_dt):
    results = []
    print(f"   [ë””ì‹œì¸ì‚¬ì´ë“œ] í‚¤ì›Œë“œ: {keyword}")
    for page in range(1, MAX_PAGES_PER_KEYWORD + 1):
        url = f"https://search.dcinside.com/post/p/{page}/sort/date/q/{quote(keyword)}"
        try:
            time.sleep(random.uniform(*POST_DELAY))
            resp = session.get(url, timeout=12)
            if resp.status_code != 200: break
            soup = BeautifulSoup(resp.text, "html.parser")
            posts = soup.select(".sch_result_list > li")
            if not posts: break
            
            for post in posts:
                date_tag, title_tag = post.select_one("span.date_time"), post.select_one("a.tit_txt")
                if not date_tag or not title_tag: continue
                p_date = parse_date(date_tag.text)
                if p_date < cutoff_dt: return results
                
                link = title_tag.get("href", "")
                if any(x in link for x in EXCLUDE_URL_KEYWORDS): continue
                if TARGET_URL_KEYWORDS and not any(x in link for x in TARGET_URL_KEYWORDS): continue
                
                body = ""
                if FETCH_BODY:
                    try:
                        m_link = link
                        m = re.search(r"id=([^&]+)&no=(\d+)", link)
                        if m: m_link = f"https://m.dcinside.com/board/{m.group(1)}/{m.group(2)}"
                        b_resp = session.get(m_link, timeout=10)
                        b_soup = BeautifulSoup(b_resp.text, "lxml")
                        body_el = b_soup.select_one("div.thum-txt, div.writing_view_box, div.thum-txtin")
                        if body_el: body = clean_text(body_el.get_text("\n"))[:BODY_MAX_CHARS]
                    except: pass

                results.append({
                    "ì›”": str(p_date.month), "ì¼": str(p_date.day), "ì»¤ë®¤ë‹ˆí‹°": "ë””ì‹œì¸ì‚¬ì´ë“œ",
                    "íƒ€ì‚¬ëª…/í‚¤ì›Œë“œ": keyword, "ì œëª©": clean_text(title_tag.text), "ë³¸ë¬¸(ë‚´ìš©)": body, "URL": link
                })
        except Exception as e:
            logger.error(f"ë””ì‹œ ì—ëŸ¬: {e}"); break
    return results

# 3-2) ì˜¤ë¥´ë¹„
def fetch_orbi(session, keyword, cutoff_dt):
    results = []
    print(f"   [ì˜¤ë¥´ë¹„] í‚¤ì›Œë“œ: {keyword}")
    url = f"https://orbi.kr/search?q={quote(keyword)}&type=post"
    try:
        time.sleep(random.uniform(*POST_DELAY))
        resp = session.get(url, timeout=12)
        if resp.status_code != 200: return results
        soup = BeautifulSoup(resp.text, "html.parser")
        # ì˜¤ë¥´ë¹„ ê²€ìƒ‰ ê²°ê³¼ ì…€ë ‰í„° ë³´ê°•
        posts = soup.select("div.search-result > div.p-m, div.item") 
        for post in posts:
            title_tag = post.select_one("a.title, a.tit")
            date_tag = post.select_one("span.date, span.time")
            if not title_tag or not date_tag: continue
            
            p_date = parse_date(date_tag.text)
            if p_date < cutoff_dt: continue
            
            link = title_tag.get("href", "")
            if link and not link.startswith("http"): link = "https://orbi.kr" + link
            
            results.append({
                "ì›”": str(p_date.month), "ì¼": str(p_date.day), "ì»¤ë®¤ë‹ˆí‹°": "ì˜¤ë¥´ë¹„",
                "íƒ€ì‚¬ëª…/í‚¤ì›Œë“œ": keyword, "ì œëª©": clean_text(title_tag.text), "ë³¸ë¬¸(ë‚´ìš©)": "", "URL": link
            })
    except Exception as e:
        logger.error(f"ì˜¤ë¥´ë¹„ ì—ëŸ¬: {e}")
    return results

# 3-3) ë„¤ì´ë²„ ì¹´í˜ (ìˆ˜ë§Œíœ˜, í¬ë§Œí•œ) - ë¡œì§ ê³ ë„í™”
def fetch_naver_cafe(session, cafe_name, cafe_id, keyword, cutoff_dt):
    results = []
    print(f"   [{cafe_name}] í‚¤ì›Œë“œ: {keyword}")
    
    # 500 ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ í—¤ë” ê°•í™” ë° URL ë³€ê²½
    cafe_headers = session.headers.copy()
    cafe_headers.update({
        "Referer": f"https://m.cafe.naver.com/ca-fe/web/cafes/{cafe_id}/search/articles?query={quote(keyword)}",
        "X-Cafe-Product": "m"
    })

    for page in range(1, 3): # ì¹´í˜ ìˆ˜ì§‘ ë²”ìœ„ ì œí•œ (ì•ˆì •ì„±)
        # ì•ˆì •ì ì¸ ëª¨ë°”ì¼ ê²€ìƒ‰ URL ì‚¬ìš©
        url = f"https://m.cafe.naver.com/ArticleSearchList.nhn?search.cafeId={cafe_id}&search.query={quote(keyword)}&search.page={page}"
        try:
            time.sleep(random.uniform(2.0, 4.0)) # ë” ë„‰ë„‰í•œ ë”œë ˆì´
            resp = session.get(url, headers=cafe_headers, timeout=15)
            
            if resp.status_code != 200:
                logger.warning(f"   [{cafe_name}] ìƒíƒœ ì½”ë“œ ë¶ˆëŸ‰: {resp.status_code}")
                break
                
            soup = BeautifulSoup(resp.text, "html.parser")
            posts = soup.select("li.board_box, ul.list_area > li")
            if not posts: 
                # ë‹¤ë¥¸ êµ¬ì¡° ì‹œë„
                posts = soup.select("div.article_list_area li")
                if not posts: break
            
            for post in posts:
                title_el = post.select_one("strong.tit, span.tit, div.tit")
                date_el = post.select_one("span.time, span.date")
                if not title_el or not date_el: continue
                
                p_date = parse_date(date_el.text)
                if p_date < cutoff_dt: return results
                
                a_tag = post.select_one("a")
                if not a_tag: continue
                link = a_tag.get("href", "")
                if link and not link.startswith("http"): link = "https://m.cafe.naver.com" + link
                
                results.append({
                    "ì›”": str(p_date.month), "ì¼": str(p_date.day), "ì»¤ë®¤ë‹ˆí‹°": cafe_name,
                    "íƒ€ì‚¬ëª…/í‚¤ì›Œë“œ": keyword, "ì œëª©": clean_text(title_el.text), "ë³¸ë¬¸(ë‚´ìš©)": "", "URL": link
                })
        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ì¹´í˜({cafe_name}) ì—ëŸ¬: {e}"); break
    return results


# =========================
# 4) ë©”ì¸ ì‹¤í–‰ ë¡œì§
# =========================
def main():
    print("\nğŸ“… ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ ì…ë ¥ (YYYY-MM-DD)")
    date_input = input("ğŸ‘‰ ì…ë ¥: ").strip()
    try:
        cutoff_dt = datetime.strptime(date_input, "%Y-%m-%d")
    except:
        print("âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤."); return

    session = build_session()
    final_data = []

    for kw in SEARCH_KEYWORDS:
        logger.info(f"ğŸš€ [{kw}] í†µí•© ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ê°€ë™...")
        
        # ê° ì‚¬ì´íŠ¸ë³„ ìˆ˜ì§‘ ê¸°ëŠ¥ì„ ë…ë¦½ì ì¸ try-exceptë¡œ ê°ì‹¸ ì•ˆì •ì„± í™•ë³´
        try:
            final_data.extend(fetch_dc_inside(session, kw, cutoff_dt))
        except Exception as e:
            logger.error(f"ë””ì‹œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        try:
            final_data.extend(fetch_orbi(session, kw, cutoff_dt))
        except Exception as e:
            logger.error(f"ì˜¤ë¥´ë¹„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        for name, cid in NAVER_CAFES.items():
            try:
                final_data.extend(fetch_naver_cafe(session, name, cid, kw, cutoff_dt))
            except Exception as e:
                logger.error(f"{name} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    if not final_data:
        print("\nâš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ìµœì¢…ì ìœ¼ë¡œ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."); return

    df = pd.DataFrame(final_data).drop_duplicates(subset=["URL"])
    print(f"\nâœ… í•„í„°ë§ í›„ ì´ {len(df)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ (ì¤‘ë³µ ì œì™¸)")

    # êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥
    try:
        print("\nğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì‹œë„ ì¤‘...")
        gc = connect_gsheet()
        spreadsheet_id = extract_spreadsheet_id(SPREADSHEET_URL)
        target_gid = extract_gid(SPREADSHEET_URL)
        sh = gc.open_by_key(spreadsheet_id)
        
        # GIDê°€ ìˆìœ¼ë©´ IDë¡œ, ì—†ìœ¼ë©´ ì´ë¦„ìœ¼ë¡œ ì‹œíŠ¸ ì„ íƒ
        ws = None
        if target_gid:
            try:
                ws = sh.get_worksheet_by_id(int(target_gid))
                logger.info(f"ï¿½ GID({target_gid})ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œíŠ¸ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤: '{ws.title}'")
            except:
                logger.warning(f"âš ï¸ GID({target_gid})ë¡œ ì‹œíŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì´ë¦„('{INPUT_SHEET_NAME}')ìœ¼ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
        
        if not ws:
            ws = sh.worksheet(INPUT_SHEET_NAME)

        ensure_header(ws, HEADERS)
        
        # ë°ì´í„° ì—…ë¡œë“œ
        df_upload = df[HEADERS].fillna("").astype(str)
        if not df_upload.empty:
            print(f"â¬†ï¸ '{ws.title}' ì‹œíŠ¸ì— {len(df_upload)}ê±´ì˜ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•©ë‹ˆë‹¤...")
            last_row = len(ws.get_all_values()) + 1
            ws.update(df_upload.values.tolist(), f"A{last_row}")
            logger.info(f"ğŸš€ êµ¬ê¸€ ì‹œíŠ¸ ì—…ë¡œë“œ ì„±ê³µ! ({ws.title}) ì´ {last_row + len(df_upload) - 1}í–‰ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            logger.info("âš ï¸ ì—…ë¡œë“œí•  ìˆ˜ì§‘ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        print(f"   (ë„ì›€ë§: Colabì—ì„œ êµ¬ê¸€ ê³„ì • ì¸ì¦ íŒì—…ì´ ë–´ëŠ”ì§€, ì‹œíŠ¸ í¸ì§‘ ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.)")

if __name__ == "__main__":
    main()
